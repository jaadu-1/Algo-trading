{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 12343702,
          "sourceType": "datasetVersion",
          "datasetId": 7781671
        },
        {
          "sourceId": 12348534,
          "sourceType": "datasetVersion",
          "datasetId": 7784824
        }
      ],
      "dockerImageVersionId": 31040,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "notebook674cd42c99",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaadu-1/Algo-trading/blob/main/notebook674cd42c99.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "gRxzWWOUYJ6d"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "krish39696_submission_path = kagglehub.dataset_download('krish39696/submission')\n",
        "krish39696_krish_data_path = kagglehub.dataset_download('krish39696/krish-data')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "jxoDA7etYJ6h"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lightgbm\n",
        "!pip install pyarrow --quiet\n",
        "\n",
        "import pandas as pd\n",
        "import pyarrow.parquet as pq\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-03T18:11:47.433198Z",
          "iopub.execute_input": "2025-07-03T18:11:47.433486Z",
          "iopub.status.idle": "2025-07-03T18:12:00.454302Z",
          "shell.execute_reply.started": "2025-07-03T18:11:47.433464Z",
          "shell.execute_reply": "2025-07-03T18:12:00.45342Z"
        },
        "id": "FlXY-zLVYJ6i",
        "outputId": "4c7ba8ab-66df-487c-b6cf-b51f175d8788"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (4.5.0)\nRequirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from lightgbm) (1.26.4)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from lightgbm) (1.15.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.0->lightgbm) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.0->lightgbm) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.0->lightgbm) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.0->lightgbm) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.0->lightgbm) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.0->lightgbm) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.0->lightgbm) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.0->lightgbm) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17.0->lightgbm) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17.0->lightgbm) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17.0->lightgbm) (2024.2.0)\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Base data path\n",
        "DATA_PATH = \"/kaggle/input/krish-data\"\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-03T18:12:00.455693Z",
          "iopub.execute_input": "2025-07-03T18:12:00.45633Z",
          "iopub.status.idle": "2025-07-03T18:12:00.460734Z",
          "shell.execute_reply.started": "2025-07-03T18:12:00.456302Z",
          "shell.execute_reply": "2025-07-03T18:12:00.45986Z"
        },
        "id": "yc1incBDYJ6j"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Core datasets\n",
        "train = pd.read_parquet(f\"{DATA_PATH}/train_data.parquet\")\n",
        "test = pd.read_parquet(f\"{DATA_PATH}/test_data.parquet\")\n",
        "submission = pd.read_csv(f\"{DATA_PATH}/685404e30cfdb_submission_template.csv\")\n",
        "\n",
        "# Supplementary files\n",
        "events = pd.read_parquet(f\"{DATA_PATH}/add_event.parquet\")\n",
        "offers = pd.read_parquet(f\"{DATA_PATH}/offer_metadata.parquet\")\n",
        "data_dict = pd.read_csv(f\"{DATA_PATH}/data_dictionary.csv\")\n",
        "\n",
        "# This file was uploaded separately\n",
        "transactions = pd.read_parquet(\"/kaggle/input/krish-data/add_trans.parquet\")\n",
        "\n",
        "print(\"✅ All 7 datasets loaded successfully.\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-03T18:12:00.461618Z",
          "iopub.execute_input": "2025-07-03T18:12:00.461877Z",
          "iopub.status.idle": "2025-07-03T18:12:46.874415Z",
          "shell.execute_reply.started": "2025-07-03T18:12:00.461816Z",
          "shell.execute_reply": "2025-07-03T18:12:46.873415Z"
        },
        "id": "oXRRS9xuYJ6j",
        "outputId": "1501eb52-36da-4ee4-f94e-c3dfafd62325"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "✅ All 7 datasets loaded successfully.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================\n",
        "# STEP X: Preprocessing\n",
        "# ====================\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Separate column types\n",
        "num_cols = train.select_dtypes(include=['float64', 'int64']).columns\n",
        "cat_cols = train.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Ensure we drop the target variable if it's in cat_cols\n",
        "if 'y' in cat_cols:\n",
        "    cat_cols = cat_cols.drop('y')\n",
        "\n",
        "# Handle numeric columns (fill missing with mean)\n",
        "for col in num_cols:\n",
        "    if col != 'y':  # don't fill target column\n",
        "        train[col] = train[col].fillna(train[col].mean())\n",
        "        test[col] = test[col].fillna(train[col].mean())\n",
        "\n",
        "# Handle categorical columns (label encoding)\n",
        "for col in cat_cols:\n",
        "    if col in test.columns:\n",
        "        le = LabelEncoder()\n",
        "        combined = pd.concat([train[col], test[col]], axis=0).astype(str)\n",
        "        le.fit(combined)\n",
        "        train[col] = le.transform(train[col].astype(str))\n",
        "        test[col] = le.transform(test[col].astype(str))\n",
        "    else:\n",
        "        print(f\"⚠️ Skipping {col} - not found in test set.\")\n",
        "\n",
        "print(\"✅ Preprocessing done.\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-03T18:12:46.87635Z",
          "iopub.execute_input": "2025-07-03T18:12:46.876616Z",
          "iopub.status.idle": "2025-07-03T18:15:15.319129Z",
          "shell.execute_reply.started": "2025-07-03T18:12:46.876593Z",
          "shell.execute_reply": "2025-07-03T18:15:15.318182Z"
        },
        "id": "IckOJO0FYJ6k",
        "outputId": "d4193220-5946-4d49-e680-a206ca0c4e66"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "✅ Preprocessing done.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Get only numerical columns (excluding 'y' if present)\n",
        "num_cols = train.select_dtypes(include=['float64', 'int64']).columns.drop('y', errors='ignore')\n",
        "\n",
        "# Fit: train only\n",
        "train['num_mean'] = train[num_cols].mean(axis=1)\n",
        "train['num_std'] = train[num_cols].std(axis=1)\n",
        "train['num_min'] = train[num_cols].min(axis=1)\n",
        "train['num_max'] = train[num_cols].max(axis=1)\n",
        "\n",
        "# Apply same columns to test\n",
        "test['num_mean'] = test[num_cols].mean(axis=1)\n",
        "test['num_std'] = test[num_cols].std(axis=1)\n",
        "test['num_min'] = test[num_cols].min(axis=1)\n",
        "test['num_max'] = test[num_cols].max(axis=1)\n",
        "\n",
        "print(\"✅ Basic row-wise statistical features added.\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-03T18:15:15.320013Z",
          "iopub.execute_input": "2025-07-03T18:15:15.320279Z",
          "iopub.status.idle": "2025-07-03T18:15:39.875823Z",
          "shell.execute_reply.started": "2025-07-03T18:15:15.320246Z",
          "shell.execute_reply": "2025-07-03T18:15:39.874961Z"
        },
        "id": "9cvv3VljYJ6k",
        "outputId": "aa109f61-caaf-4ae7-8b7f-d995626ce47f"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/tmp/ipykernel_146/1123730705.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  train['num_mean'] = train[num_cols].mean(axis=1)\n/tmp/ipykernel_146/1123730705.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  train['num_std'] = train[num_cols].std(axis=1)\n/tmp/ipykernel_146/1123730705.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  train['num_min'] = train[num_cols].min(axis=1)\n/tmp/ipykernel_146/1123730705.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  train['num_max'] = train[num_cols].max(axis=1)\n/tmp/ipykernel_146/1123730705.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  test['num_mean'] = test[num_cols].mean(axis=1)\n/tmp/ipykernel_146/1123730705.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  test['num_std'] = test[num_cols].std(axis=1)\n/tmp/ipykernel_146/1123730705.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  test['num_min'] = test[num_cols].min(axis=1)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "✅ Basic row-wise statistical features added.\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/tmp/ipykernel_146/1123730705.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  test['num_max'] = test[num_cols].max(axis=1)\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply only if categorical exists\n",
        "if 'id2' in train.columns:\n",
        "    freq_map = train['id2'].value_counts().to_dict()\n",
        "    train['id2_freq'] = train['id2'].map(freq_map)\n",
        "    test['id2_freq'] = test['id2'].map(freq_map)\n",
        "    print(\"✅ Frequency encoding for id2 added.\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-03T18:15:39.87667Z",
          "iopub.execute_input": "2025-07-03T18:15:39.876987Z",
          "iopub.status.idle": "2025-07-03T18:15:39.987057Z",
          "shell.execute_reply.started": "2025-07-03T18:15:39.876956Z",
          "shell.execute_reply": "2025-07-03T18:15:39.986109Z"
        },
        "id": "UKgWmWMdYJ6l",
        "outputId": "908802e9-442c-413b-8054-89fead79b53a"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "✅ Frequency encoding for id2 added.\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/tmp/ipykernel_146/3051208027.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  train['id2_freq'] = train['id2'].map(freq_map)\n/tmp/ipykernel_146/3051208027.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  test['id2_freq'] = test['id2'].map(freq_map)\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# === Create aggregate sum feature from f23 to f27 ===\n",
        "cols_to_sum = [f\"f{i}\" for i in range(23, 28) if f\"f{i}\" in train.columns]\n",
        "\n",
        "train[\"f23_f27_sum\"] = train[cols_to_sum].sum(axis=1)\n",
        "test[\"f23_f27_sum\"] = test[cols_to_sum].sum(axis=1)\n",
        "\n",
        "print(\"✅ Created feature: f23_f27_sum\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-03T18:15:39.987972Z",
          "iopub.execute_input": "2025-07-03T18:15:39.988234Z",
          "iopub.status.idle": "2025-07-03T18:15:40.19973Z",
          "shell.execute_reply.started": "2025-07-03T18:15:39.988206Z",
          "shell.execute_reply": "2025-07-03T18:15:40.198868Z"
        },
        "id": "yV9g1JuxYJ6l",
        "outputId": "683d8602-82ad-48c6-8fcd-00121bfcbb7c"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/tmp/ipykernel_146/2396716883.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  train[\"f23_f27_sum\"] = train[cols_to_sum].sum(axis=1)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "✅ Created feature: f23_f27_sum\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/tmp/ipykernel_146/2396716883.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  test[\"f23_f27_sum\"] = test[cols_to_sum].sum(axis=1)\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# === Feature Engineering: Interest Scores (f1 to f12) ===\n",
        "interest_cols = [f\"f{i}\" for i in range(1, 13) if f\"f{i}\" in train.columns]\n",
        "\n",
        "# Sum, mean, std, max, min, range\n",
        "for df in [train, test]:\n",
        "    df[\"interest_sum\"] = df[interest_cols].sum(axis=1)\n",
        "    df[\"interest_mean\"] = df[interest_cols].mean(axis=1)\n",
        "    df[\"interest_max\"] = df[interest_cols].max(axis=1)\n",
        "    df[\"interest_min\"] = df[interest_cols].min(axis=1)\n",
        "    df[\"interest_std\"] = df[interest_cols].std(axis=1)\n",
        "    df[\"interest_range\"] = df[\"interest_max\"] - df[\"interest_min\"]\n",
        "\n",
        "    # Find index of top topic (f1 → 1, f2 → 2, ...)\n",
        "    df[\"top_topic_idx\"] = df[interest_cols].idxmax(axis=1).str.extract(r\"f(\\d+)\").astype(float)\n",
        "\n",
        "print(\"✅ Created features based on interest scores (f1 to f12).\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-03T18:15:40.20112Z",
          "iopub.execute_input": "2025-07-03T18:15:40.201468Z",
          "iopub.status.idle": "2025-07-03T18:15:44.240787Z",
          "shell.execute_reply.started": "2025-07-03T18:15:40.201433Z",
          "shell.execute_reply": "2025-07-03T18:15:44.239814Z"
        },
        "id": "kgS-k8tQYJ6m",
        "outputId": "90ebf671-7a91-4061-9032-9a2d32c4e246"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/tmp/ipykernel_146/2624438021.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[\"interest_sum\"] = df[interest_cols].sum(axis=1)\n/tmp/ipykernel_146/2624438021.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[\"interest_mean\"] = df[interest_cols].mean(axis=1)\n/tmp/ipykernel_146/2624438021.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[\"interest_max\"] = df[interest_cols].max(axis=1)\n/tmp/ipykernel_146/2624438021.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[\"interest_min\"] = df[interest_cols].min(axis=1)\n/tmp/ipykernel_146/2624438021.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[\"interest_std\"] = df[interest_cols].std(axis=1)\n/tmp/ipykernel_146/2624438021.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[\"interest_range\"] = df[\"interest_max\"] - df[\"interest_min\"]\n/tmp/ipykernel_146/2624438021.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[\"top_topic_idx\"] = df[interest_cols].idxmax(axis=1).str.extract(r\"f(\\d+)\").astype(float)\n/tmp/ipykernel_146/2624438021.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[\"interest_sum\"] = df[interest_cols].sum(axis=1)\n/tmp/ipykernel_146/2624438021.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[\"interest_mean\"] = df[interest_cols].mean(axis=1)\n/tmp/ipykernel_146/2624438021.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[\"interest_max\"] = df[interest_cols].max(axis=1)\n/tmp/ipykernel_146/2624438021.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[\"interest_min\"] = df[interest_cols].min(axis=1)\n/tmp/ipykernel_146/2624438021.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[\"interest_std\"] = df[interest_cols].std(axis=1)\n/tmp/ipykernel_146/2624438021.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[\"interest_range\"] = df[\"interest_max\"] - df[\"interest_min\"]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "✅ Created features based on interest scores (f1 to f12).\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/tmp/ipykernel_146/2624438021.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[\"top_topic_idx\"] = df[interest_cols].idxmax(axis=1).str.extract(r\"f(\\d+)\").astype(float)\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# === Feature Engineering: Engagement Score (f14 to f21) ===\n",
        "engagement_cols = [f\"f{i}\" for i in range(14, 22) if f\"f{i}\" in train.columns]\n",
        "\n",
        "for df in [train, test]:\n",
        "    df[\"engagement_score\"] = df[engagement_cols].sum(axis=1)\n",
        "\n",
        "print(\"✅ Created engagement_score from f14 to f21.\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-03T18:15:44.241885Z",
          "iopub.execute_input": "2025-07-03T18:15:44.242186Z",
          "iopub.status.idle": "2025-07-03T18:15:44.455066Z",
          "shell.execute_reply.started": "2025-07-03T18:15:44.242154Z",
          "shell.execute_reply": "2025-07-03T18:15:44.454135Z"
        },
        "id": "eALdAQAGYJ6m",
        "outputId": "9c3a4d80-d2b8-4b0f-852b-0f3f4d811ffa"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/tmp/ipykernel_146/1890997656.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[\"engagement_score\"] = df[engagement_cols].sum(axis=1)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "✅ Created engagement_score from f14 to f21.\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/tmp/ipykernel_146/1890997656.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[\"engagement_score\"] = df[engagement_cols].sum(axis=1)\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# === Feature Engineering: Combined Feature from f39 to f75 ===\n",
        "block_39_75_cols = [f\"f{i}\" for i in range(39, 76) if f\"f{i}\" in train.columns]\n",
        "\n",
        "for df in [train, test]:\n",
        "    df[\"feature_block_39_75\"] = df[block_39_75_cols].sum(axis=1)\n",
        "\n",
        "print(\"✅ Created feature_block_39_75 from f39 to f75.\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-03T18:15:44.457767Z",
          "iopub.execute_input": "2025-07-03T18:15:44.458054Z",
          "iopub.status.idle": "2025-07-03T18:15:44.994422Z",
          "shell.execute_reply.started": "2025-07-03T18:15:44.458033Z",
          "shell.execute_reply": "2025-07-03T18:15:44.993314Z"
        },
        "id": "r4dJo214YJ6m",
        "outputId": "5deb5f4f-cca3-4b3e-b953-50073bdc63bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "✅ Created feature_block_39_75 from f39 to f75.\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/tmp/ipykernel_146/2087079204.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[\"feature_block_39_75\"] = df[block_39_75_cols].sum(axis=1)\n/tmp/ipykernel_146/2087079204.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[\"feature_block_39_75\"] = df[block_39_75_cols].sum(axis=1)\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Step: Time spent features\n",
        "time_cols = [f\"f{i}\" for i in range(59, 76) if i != 67]\n",
        "\n",
        "# 1. Total time spent\n",
        "train[\"total_time_spent\"] = train[time_cols].sum(axis=1)\n",
        "test[\"total_time_spent\"] = test[time_cols].sum(axis=1)\n",
        "\n",
        "# 2. Mean time spent\n",
        "train[\"mean_time_spent\"] = train[time_cols].mean(axis=1)\n",
        "test[\"mean_time_spent\"] = test[time_cols].mean(axis=1)\n",
        "\n",
        "# 3. Standard deviation of time spent\n",
        "train[\"std_time_spent\"] = train[time_cols].std(axis=1)\n",
        "test[\"std_time_spent\"] = test[time_cols].std(axis=1)\n",
        "\n",
        "# 4. Max time spent on any activity\n",
        "train[\"max_time_spent\"] = train[time_cols].max(axis=1)\n",
        "test[\"max_time_spent\"] = test[time_cols].max(axis=1)\n",
        "\n",
        "# 5. Time spent range (max - min)\n",
        "train[\"range_time_spent\"] = train[time_cols].max(axis=1) - train[time_cols].min(axis=1)\n",
        "test[\"range_time_spent\"] = test[time_cols].max(axis=1) - test[time_cols].min(axis=1)\n",
        "\n",
        "print(\"✅ 5 time-spent features created from f59-f75 (excluding f67).\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-03T18:15:44.995418Z",
          "iopub.execute_input": "2025-07-03T18:15:44.995742Z",
          "iopub.status.idle": "2025-07-03T18:15:46.869977Z",
          "shell.execute_reply.started": "2025-07-03T18:15:44.995713Z",
          "shell.execute_reply": "2025-07-03T18:15:46.868845Z"
        },
        "id": "pW4ifTmtYJ6n",
        "outputId": "45701391-0bc3-416d-89fb-f8974418f173"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/tmp/ipykernel_146/2795891540.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  train[\"total_time_spent\"] = train[time_cols].sum(axis=1)\n/tmp/ipykernel_146/2795891540.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  test[\"total_time_spent\"] = test[time_cols].sum(axis=1)\n/tmp/ipykernel_146/2795891540.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  train[\"mean_time_spent\"] = train[time_cols].mean(axis=1)\n/tmp/ipykernel_146/2795891540.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  test[\"mean_time_spent\"] = test[time_cols].mean(axis=1)\n/tmp/ipykernel_146/2795891540.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  train[\"std_time_spent\"] = train[time_cols].std(axis=1)\n/tmp/ipykernel_146/2795891540.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  test[\"std_time_spent\"] = test[time_cols].std(axis=1)\n/tmp/ipykernel_146/2795891540.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  train[\"max_time_spent\"] = train[time_cols].max(axis=1)\n/tmp/ipykernel_146/2795891540.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  test[\"max_time_spent\"] = test[time_cols].max(axis=1)\n/tmp/ipykernel_146/2795891540.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  train[\"range_time_spent\"] = train[time_cols].max(axis=1) - train[time_cols].min(axis=1)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "✅ 5 time-spent features created from f59-f75 (excluding f67).\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/tmp/ipykernel_146/2795891540.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  test[\"range_time_spent\"] = test[time_cols].max(axis=1) - test[time_cols].min(axis=1)\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Step: Ratio-based features\n",
        "ratio_cols = [f\"f{i}\" for i in range(78, 94)]\n",
        "\n",
        "# 1. Sum of ratios\n",
        "train[\"sum_ratios\"] = train[ratio_cols].sum(axis=1)\n",
        "test[\"sum_ratios\"] = test[ratio_cols].sum(axis=1)\n",
        "\n",
        "# 2. Mean of ratios\n",
        "train[\"mean_ratio\"] = train[ratio_cols].mean(axis=1)\n",
        "test[\"mean_ratio\"] = test[ratio_cols].mean(axis=1)\n",
        "\n",
        "# 3. Max ratio value\n",
        "train[\"max_ratio\"] = train[ratio_cols].max(axis=1)\n",
        "test[\"max_ratio\"] = test[ratio_cols].max(axis=1)\n",
        "\n",
        "# 4. Std deviation of ratios\n",
        "train[\"std_ratio\"] = train[ratio_cols].std(axis=1)\n",
        "test[\"std_ratio\"] = test[ratio_cols].std(axis=1)\n",
        "\n",
        "# 5. Ratio spread (max - min)\n",
        "train[\"ratio_spread\"] = train[ratio_cols].max(axis=1) - train[ratio_cols].min(axis=1)\n",
        "test[\"ratio_spread\"] = test[ratio_cols].max(axis=1) - test[ratio_cols].min(axis=1)\n",
        "\n",
        "print(\"✅ 5 ratio-based features created from f78 to f93.\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-03T18:15:46.870813Z",
          "iopub.execute_input": "2025-07-03T18:15:46.871147Z",
          "iopub.status.idle": "2025-07-03T18:15:48.794668Z",
          "shell.execute_reply.started": "2025-07-03T18:15:46.871116Z",
          "shell.execute_reply": "2025-07-03T18:15:48.79382Z"
        },
        "id": "_dqVSUM3YJ6n",
        "outputId": "7b2a763d-bca0-4bb7-d55b-db1376b39e6f"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/tmp/ipykernel_146/2845027568.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  train[\"sum_ratios\"] = train[ratio_cols].sum(axis=1)\n/tmp/ipykernel_146/2845027568.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  test[\"sum_ratios\"] = test[ratio_cols].sum(axis=1)\n/tmp/ipykernel_146/2845027568.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  train[\"mean_ratio\"] = train[ratio_cols].mean(axis=1)\n/tmp/ipykernel_146/2845027568.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  test[\"mean_ratio\"] = test[ratio_cols].mean(axis=1)\n/tmp/ipykernel_146/2845027568.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  train[\"max_ratio\"] = train[ratio_cols].max(axis=1)\n/tmp/ipykernel_146/2845027568.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  test[\"max_ratio\"] = test[ratio_cols].max(axis=1)\n/tmp/ipykernel_146/2845027568.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  train[\"std_ratio\"] = train[ratio_cols].std(axis=1)\n/tmp/ipykernel_146/2845027568.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  test[\"std_ratio\"] = test[ratio_cols].std(axis=1)\n/tmp/ipykernel_146/2845027568.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  train[\"ratio_spread\"] = train[ratio_cols].max(axis=1) - train[ratio_cols].min(axis=1)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "✅ 5 ratio-based features created from f78 to f93.\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/tmp/ipykernel_146/2845027568.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  test[\"ratio_spread\"] = test[ratio_cols].max(axis=1) - test[ratio_cols].min(axis=1)\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Step: Behavioral category features\n",
        "behavior_cols = [f\"f{i}\" for i in range(94, 114)]\n",
        "\n",
        "# 1. Sum of behavioral scores (activity intensity)\n",
        "train[\"behavior_sum\"] = train[behavior_cols].sum(axis=1)\n",
        "test[\"behavior_sum\"] = test[behavior_cols].sum(axis=1)\n",
        "\n",
        "# 2. Mean behavioral score\n",
        "train[\"behavior_mean\"] = train[behavior_cols].mean(axis=1)\n",
        "test[\"behavior_mean\"] = test[behavior_cols].mean(axis=1)\n",
        "\n",
        "# 3. Count of non-zero behaviors (how many categories are active)\n",
        "train[\"behavior_active_count\"] = (train[behavior_cols] != 0).sum(axis=1)\n",
        "test[\"behavior_active_count\"] = (test[behavior_cols] != 0).sum(axis=1)\n",
        "\n",
        "# 4. Max behavioral score (dominant behavior strength)\n",
        "train[\"behavior_max\"] = train[behavior_cols].max(axis=1)\n",
        "test[\"behavior_max\"] = test[behavior_cols].max(axis=1)\n",
        "\n",
        "# 5. Binary behavior pattern (is any one behavior dominant)\n",
        "train[\"behavior_high_variance\"] = train[behavior_cols].std(axis=1) > 1.0\n",
        "test[\"behavior_high_variance\"] = test[behavior_cols].std(axis=1) > 1.0\n",
        "\n",
        "print(\"✅ 5 behavioral features created from f94 to f113.\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-03T18:15:48.795572Z",
          "iopub.execute_input": "2025-07-03T18:15:48.795877Z",
          "iopub.status.idle": "2025-07-03T18:15:50.697371Z",
          "shell.execute_reply.started": "2025-07-03T18:15:48.795847Z",
          "shell.execute_reply": "2025-07-03T18:15:50.696462Z"
        },
        "id": "41LFiu6WYJ6n",
        "outputId": "945e1004-d91a-45b0-aaf9-582d7ae7f3f1"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/tmp/ipykernel_146/2739766076.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  train[\"behavior_sum\"] = train[behavior_cols].sum(axis=1)\n/tmp/ipykernel_146/2739766076.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  test[\"behavior_sum\"] = test[behavior_cols].sum(axis=1)\n/tmp/ipykernel_146/2739766076.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  train[\"behavior_mean\"] = train[behavior_cols].mean(axis=1)\n/tmp/ipykernel_146/2739766076.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  test[\"behavior_mean\"] = test[behavior_cols].mean(axis=1)\n/tmp/ipykernel_146/2739766076.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  train[\"behavior_active_count\"] = (train[behavior_cols] != 0).sum(axis=1)\n/tmp/ipykernel_146/2739766076.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  test[\"behavior_active_count\"] = (test[behavior_cols] != 0).sum(axis=1)\n/tmp/ipykernel_146/2739766076.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  train[\"behavior_max\"] = train[behavior_cols].max(axis=1)\n/tmp/ipykernel_146/2739766076.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  test[\"behavior_max\"] = test[behavior_cols].max(axis=1)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "✅ 5 behavioral features created from f94 to f113.\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/tmp/ipykernel_146/2739766076.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  train[\"behavior_high_variance\"] = train[behavior_cols].std(axis=1) > 1.0\n/tmp/ipykernel_146/2739766076.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  test[\"behavior_high_variance\"] = test[behavior_cols].std(axis=1) > 1.0\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# Logical Feature Engineering: Ratios, Clicks, CTRs (Safe Version)\n",
        "# ================================\n",
        "\n",
        "# 1. Define relevant columns\n",
        "ratio_cols = [f\"f{i}\" for i in range(113, 123)]\n",
        "click_cols = [f\"f{i}\" for i in range(124, 130)]\n",
        "ctr_cols = [f\"f{i}\" for i in range(130, 139)]\n",
        "\n",
        "# 2. Create logical derived features\n",
        "def create_logical_features(df):\n",
        "    features = {}\n",
        "\n",
        "    # --- Ratio Features ---\n",
        "    features[\"ratio_mean\"] = df[ratio_cols].mean(axis=1)\n",
        "    features[\"ratio_std\"] = df[ratio_cols].std(axis=1)\n",
        "    features[\"ratio_max\"] = df[ratio_cols].max(axis=1)\n",
        "    features[\"ratio_min\"] = df[ratio_cols].min(axis=1)\n",
        "    features[\"ratio_range\"] = features[\"ratio_max\"] - features[\"ratio_min\"]\n",
        "\n",
        "    # --- Click Features ---\n",
        "    features[\"total_clicks\"] = df[click_cols].sum(axis=1)\n",
        "    features[\"click_mean\"] = df[click_cols].mean(axis=1)\n",
        "    features[\"click_std\"] = df[click_cols].std(axis=1)\n",
        "    features[\"click_max\"] = df[click_cols].max(axis=1)\n",
        "\n",
        "    # --- CTR Features ---\n",
        "    features[\"ctr_mean\"] = df[ctr_cols].mean(axis=1)\n",
        "    features[\"ctr_std\"] = df[ctr_cols].std(axis=1)\n",
        "    features[\"ctr_max\"] = df[ctr_cols].max(axis=1)\n",
        "    features[\"ctr_min\"] = df[ctr_cols].min(axis=1)\n",
        "\n",
        "    # --- Interaction Features ---\n",
        "    features[\"click_to_ratio\"] = features[\"total_clicks\"] / (features[\"ratio_mean\"] + 1e-5)\n",
        "    features[\"ctr_to_ratio\"] = features[\"ctr_mean\"] / (features[\"ratio_mean\"] + 1e-5)\n",
        "    features[\"engagement_score\"] = (\n",
        "        0.5 * features[\"total_clicks\"] +\n",
        "        0.3 * features[\"ctr_mean\"] +\n",
        "        0.2 * features[\"ratio_mean\"]\n",
        "    )\n",
        "\n",
        "    features[\"ctr_spread\"] = features[\"ctr_max\"] - features[\"ctr_min\"]\n",
        "    features[\"ctr_to_clicks\"] = features[\"ctr_mean\"] / (features[\"total_clicks\"] + 1e-5)\n",
        "\n",
        "    return pd.DataFrame(features)\n",
        "\n",
        "# 3. Reset index for clean concat\n",
        "train = train.reset_index(drop=True)\n",
        "test = test.reset_index(drop=True)\n",
        "\n",
        "# 4. Apply feature creation\n",
        "train_new_feats = create_logical_features(train)\n",
        "test_new_feats = create_logical_features(test)\n",
        "\n",
        "# 5. Concatenate without warning or index issues\n",
        "train = pd.concat([train, train_new_feats], axis=1)\n",
        "test = pd.concat([test, test_new_feats], axis=1)\n",
        "\n",
        "print(\"✅ Logical CTR/Click/Ratio features added (safe and aligned)\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-03T18:15:50.698358Z",
          "iopub.execute_input": "2025-07-03T18:15:50.698629Z",
          "iopub.status.idle": "2025-07-03T18:15:58.929524Z",
          "shell.execute_reply.started": "2025-07-03T18:15:50.698609Z",
          "shell.execute_reply": "2025-07-03T18:15:58.928686Z"
        },
        "id": "2d3XTo0sYJ6n",
        "outputId": "77fb6bdc-c227-479f-8e1f-ba36d9b4f50b"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "✅ Logical CTR/Click/Ratio features added (safe and aligned)\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# =============================\n",
        "# STEP: Ratio Features (f152/f174 to f162/f183)\n",
        "# =============================\n",
        "\n",
        "# Define numerator and denominator column lists\n",
        "numerator_cols = [f\"f{i}\" for i in range(152, 163)]   # f152 to f162\n",
        "denominator_cols = [f\"f{i}\" for i in range(174, 185)] # f174 to f184 (need 11 total)\n",
        "\n",
        "# Safety check\n",
        "assert len(numerator_cols) == len(denominator_cols), \"Mismatch in ratio feature count.\"\n",
        "\n",
        "def make_ratio_features(df):\n",
        "    # 1. Compute ratios\n",
        "    ratio_df = pd.DataFrame()\n",
        "    for num_col, den_col in zip(numerator_cols, denominator_cols):\n",
        "        ratio_df[f\"{num_col}_over_{den_col}\"] = df[num_col] / (df[den_col] + 1e-5)\n",
        "\n",
        "    # 2. Create aggregate features from all 11 ratios\n",
        "    agg_feats = pd.DataFrame()\n",
        "    agg_feats[\"ratio_block_mean\"] = ratio_df.mean(axis=1)\n",
        "    agg_feats[\"ratio_block_std\"] = ratio_df.std(axis=1)\n",
        "    agg_feats[\"ratio_block_max\"] = ratio_df.max(axis=1)\n",
        "    agg_feats[\"ratio_block_min\"] = ratio_df.min(axis=1)\n",
        "    agg_feats[\"ratio_block_sum\"] = ratio_df.sum(axis=1)\n",
        "    agg_feats[\"ratio_block_skew\"] = ratio_df.skew(axis=1)\n",
        "    agg_feats[\"ratio_block_kurt\"] = ratio_df.kurtosis(axis=1)\n",
        "\n",
        "    # 3. Top 3, Bottom 3 features\n",
        "    agg_feats[\"top1_ratio\"] = ratio_df.apply(lambda x: np.sort(x)[-1], axis=1)\n",
        "    agg_feats[\"top2_ratio\"] = ratio_df.apply(lambda x: np.sort(x)[-2], axis=1)\n",
        "    agg_feats[\"top3_ratio\"] = ratio_df.apply(lambda x: np.sort(x)[-3], axis=1)\n",
        "    agg_feats[\"bottom1_ratio\"] = ratio_df.apply(lambda x: np.sort(x)[0], axis=1)\n",
        "    agg_feats[\"bottom2_ratio\"] = ratio_df.apply(lambda x: np.sort(x)[1], axis=1)\n",
        "    agg_feats[\"bottom3_ratio\"] = ratio_df.apply(lambda x: np.sort(x)[2], axis=1)\n",
        "\n",
        "    # 4. Quantiles\n",
        "    agg_feats[\"ratio_q25\"] = ratio_df.quantile(0.25, axis=1)\n",
        "    agg_feats[\"ratio_q50\"] = ratio_df.quantile(0.50, axis=1)\n",
        "    agg_feats[\"ratio_q75\"] = ratio_df.quantile(0.75, axis=1)\n",
        "\n",
        "    # 5. Range and ratio between top and bottom\n",
        "    agg_feats[\"ratio_range\"] = agg_feats[\"top1_ratio\"] - agg_feats[\"bottom1_ratio\"]\n",
        "    agg_feats[\"top1_to_sum_ratio\"] = agg_feats[\"top1_ratio\"] / (agg_feats[\"ratio_block_sum\"] + 1e-5)\n",
        "\n",
        "    # 6. Add original ratios (11)\n",
        "    all_features = pd.concat([ratio_df, agg_feats], axis=1)\n",
        "\n",
        "    return all_features\n",
        "\n",
        "# Apply and add to train/test\n",
        "train_ratios = make_ratio_features(train)\n",
        "test_ratios = make_ratio_features(test)\n",
        "\n",
        "# Reset index to avoid fragmentation issues\n",
        "train = train.reset_index(drop=True)\n",
        "test = test.reset_index(drop=True)\n",
        "\n",
        "# Concatenate safely\n",
        "train = pd.concat([train, train_ratios], axis=1)\n",
        "test = pd.concat([test, test_ratios], axis=1)\n",
        "\n",
        "print(\"✅ Ratio features (f152/f174 to f162/f183) added (30+ total)\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-03T18:15:58.930402Z",
          "iopub.execute_input": "2025-07-03T18:15:58.930683Z",
          "iopub.status.idle": "2025-07-03T18:18:04.05109Z",
          "shell.execute_reply.started": "2025-07-03T18:15:58.930657Z",
          "shell.execute_reply": "2025-07-03T18:18:04.049781Z"
        },
        "id": "7QmL4aGVYJ6n",
        "outputId": "ac8db170-036d-4b7e-fe29-5ff9e063028d"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "✅ Ratio features (f152/f174 to f162/f183) added (30+ total)\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# STEP: Add feature from f226 to f309\n",
        "# =============================\n",
        "\n",
        "# List of relevant columns\n",
        "f226_to_f309 = [f\"f{i}\" for i in range(226, 310)]\n",
        "\n",
        "# Check if all columns exist\n",
        "missing_cols = [col for col in f226_to_f309 if col not in train.columns]\n",
        "if missing_cols:\n",
        "    print(f\"⚠️ Warning: These columns are missing and will be skipped: {missing_cols}\")\n",
        "    f226_to_f309 = [col for col in f226_to_f309 if col in train.columns]\n",
        "\n",
        "# Compute total sum across these features\n",
        "train[\"f226_to_f309_sum\"] = train[f226_to_f309].sum(axis=1)\n",
        "test[\"f226_to_f309_sum\"] = test[f226_to_f309].sum(axis=1)\n",
        "\n",
        "print(\"✅ Feature 'f226_to_f309_sum' added to train and test\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-03T18:18:04.053952Z",
          "iopub.execute_input": "2025-07-03T18:18:04.054354Z",
          "iopub.status.idle": "2025-07-03T18:18:04.894554Z",
          "shell.execute_reply.started": "2025-07-03T18:18:04.054293Z",
          "shell.execute_reply": "2025-07-03T18:18:04.892408Z"
        },
        "id": "jWfiwOgoYJ6o",
        "outputId": "2d7b9605-9deb-49c6-8e82-836600c52a50"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "✅ Feature 'f226_to_f309_sum' added to train and test\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# STEP: Add features from f361 to f366\n",
        "# =============================\n",
        "\n",
        "f361_366 = [f\"f{i}\" for i in range(361, 367)]\n",
        "\n",
        "# Check existence\n",
        "missing_cols = [col for col in f361_366 if col not in train.columns]\n",
        "if missing_cols:\n",
        "    print(f\"⚠️ Skipping missing columns: {missing_cols}\")\n",
        "    f361_366 = [col for col in f361_366 if col in train.columns]\n",
        "\n",
        "# Compute aggregate features\n",
        "train_f361_366 = train[f361_366]\n",
        "test_f361_366 = test[f361_366]\n",
        "\n",
        "agg_feats_train = pd.DataFrame({\n",
        "    \"f361_366_sum\": train_f361_366.sum(axis=1),\n",
        "    \"f361_366_mean\": train_f361_366.mean(axis=1),\n",
        "    \"f361_366_std\": train_f361_366.std(axis=1),\n",
        "    \"f361_366_max\": train_f361_366.max(axis=1),\n",
        "    \"f361_366_min\": train_f361_366.min(axis=1),\n",
        "})\n",
        "\n",
        "agg_feats_test = pd.DataFrame({\n",
        "    \"f361_366_sum\": test_f361_366.sum(axis=1),\n",
        "    \"f361_366_mean\": test_f361_366.mean(axis=1),\n",
        "    \"f361_366_std\": test_f361_366.std(axis=1),\n",
        "    \"f361_366_max\": test_f361_366.max(axis=1),\n",
        "    \"f361_366_min\": test_f361_366.min(axis=1),\n",
        "})\n",
        "\n",
        "# Concatenate in one go to avoid fragmentation warning\n",
        "train = pd.concat([train, agg_feats_train], axis=1)\n",
        "test = pd.concat([test, agg_feats_test], axis=1)\n",
        "\n",
        "print(\"✅ f361 to f366 features engineered and added.\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-03T18:18:04.897015Z",
          "iopub.execute_input": "2025-07-03T18:18:04.897396Z",
          "iopub.status.idle": "2025-07-03T18:18:12.339014Z",
          "shell.execute_reply.started": "2025-07-03T18:18:04.897359Z",
          "shell.execute_reply": "2025-07-03T18:18:12.337974Z"
        },
        "id": "p0Qp8RFDYJ6o",
        "outputId": "abc1587a-c751-4e0a-a130-2a5c9f5ae75e"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "✅ f361 to f366 features engineered and added.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# 🟡 ADVANCED EVENTS FEATURES (Optimized & Fixed)\n",
        "# ============================\n",
        "\n",
        "# Ensure required columns exist\n",
        "required_cols = ['id2', 'id4', 'id6', 'id7']\n",
        "missing_cols = [col for col in required_cols if col not in events.columns]\n",
        "if missing_cols:\n",
        "    print(f\"⚠️ Skipping advanced event features. Missing columns: {missing_cols}\")\n",
        "else:\n",
        "    # Convert id6 and id7 to numeric for aggregation\n",
        "    events[\"id6\"] = pd.to_numeric(events[\"id6\"], errors=\"coerce\")\n",
        "    events[\"id7\"] = pd.to_numeric(events[\"id7\"], errors=\"coerce\")\n",
        "\n",
        "    # --- 1. Count frequency of event types (id4) per user (LIMIT high-cardinality danger)\n",
        "    # Only take top 20 most common id4 values to avoid memory blow-up\n",
        "    top_id4_values = events[\"id4\"].value_counts().nlargest(20).index\n",
        "    filtered_events = events[events[\"id4\"].isin(top_id4_values)]\n",
        "\n",
        "    id4_counts = (\n",
        "        filtered_events.groupby([\"id2\", \"id4\"])\n",
        "        .size()\n",
        "        .unstack(fill_value=0)\n",
        "        .reset_index()\n",
        "    )\n",
        "    id4_counts.columns = ['id2'] + [f\"event_type_{col}\" for col in id4_counts.columns if col != 'id2']\n",
        "\n",
        "    # --- 2. Aggregate statistics for id6 and id7\n",
        "    event_stats = events.groupby(\"id2\").agg(\n",
        "        id6_mean=(\"id6\", \"mean\"),\n",
        "        id6_std=(\"id6\", \"std\"),\n",
        "        id6_sum=(\"id6\", \"sum\"),\n",
        "        id7_mean=(\"id7\", \"mean\"),\n",
        "        id7_max=(\"id7\", \"max\"),\n",
        "        id7_min=(\"id7\", \"min\")\n",
        "    ).reset_index()\n",
        "\n",
        "    # --- Merge into train and test\n",
        "    train = train.merge(id4_counts, on=\"id2\", how=\"left\")\n",
        "    test = test.merge(id4_counts, on=\"id2\", how=\"left\")\n",
        "\n",
        "    train = train.merge(event_stats, on=\"id2\", how=\"left\")\n",
        "    test = test.merge(event_stats, on=\"id2\", how=\"left\")\n",
        "\n",
        "    print(\"✅ Advanced event features (id4, id6, id7) merged successfully.\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-03T18:18:12.340197Z",
          "iopub.execute_input": "2025-07-03T18:18:12.340532Z",
          "iopub.status.idle": "2025-07-03T18:19:41.149457Z",
          "shell.execute_reply.started": "2025-07-03T18:18:12.340501Z",
          "shell.execute_reply": "2025-07-03T18:19:41.148651Z"
        },
        "id": "giOprds5YJ6o",
        "outputId": "36e67d7f-eb77-4490-d9a0-063214e9cae8"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "✅ Advanced event features (id4, id6, id7) merged successfully.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Load Offer Metadata\n",
        "offer = pd.read_parquet(\"/kaggle/input/krish-data/offer_metadata.parquet\")\n",
        "\n",
        "# Ensure 'id3' is string for merging\n",
        "offer[\"id3\"] = offer[\"id3\"].astype(str)\n",
        "train[\"id3\"] = train[\"id3\"].astype(str)\n",
        "test[\"id3\"] = test[\"id3\"].astype(str)\n",
        "\n",
        "# Select only the useful columns\n",
        "offer_sub = offer[[\"id3\", \"id9\", \"f375\", \"f376\", \"f378\"]].copy()\n",
        "\n",
        "# Convert all necessary fields to numeric\n",
        "offer_sub[\"f375\"] = pd.to_numeric(offer_sub[\"f375\"], errors=\"coerce\")\n",
        "offer_sub[\"f376\"] = pd.to_numeric(offer_sub[\"f376\"], errors=\"coerce\")\n",
        "offer_sub[\"f378\"] = pd.to_numeric(offer_sub[\"f378\"], errors=\"coerce\")\n",
        "\n",
        "# 🛠️ Feature 1: Combined discount strength (mean of f375, f376)\n",
        "offer_sub[\"discount_strength\"] = offer_sub[[\"f375\", \"f376\"]].mean(axis=1)\n",
        "\n",
        "# 🛠️ Feature 2: Use f378 directly as interest_score\n",
        "offer_sub[\"interest_score\"] = offer_sub[\"f378\"]\n",
        "\n",
        "# 🛠️ Feature 3: Encode id9 (offer group)\n",
        "offer_sub[\"offer_group\"] = offer_sub[\"id9\"].astype(str)\n",
        "\n",
        "# Optional Encoding\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le_offer = LabelEncoder()\n",
        "offer_sub[\"offer_group_encoded\"] = le_offer.fit_transform(offer_sub[\"offer_group\"])\n",
        "\n",
        "# Final cleanup: Drop redundant\n",
        "offer_features = offer_sub[[\"id3\", \"discount_strength\", \"interest_score\", \"offer_group_encoded\"]]\n",
        "\n",
        "# 🔁 Merge with train/test\n",
        "train = train.merge(offer_features, on=\"id3\", how=\"left\")\n",
        "test = test.merge(offer_features, on=\"id3\", how=\"left\")\n",
        "\n",
        "print(\"✅ Clean offer features extracted and merged.\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-03T18:19:41.150414Z",
          "iopub.execute_input": "2025-07-03T18:19:41.150677Z",
          "iopub.status.idle": "2025-07-03T18:19:46.312529Z",
          "shell.execute_reply.started": "2025-07-03T18:19:41.15065Z",
          "shell.execute_reply": "2025-07-03T18:19:46.311508Z"
        },
        "id": "ePgPtLiHYJ6o",
        "outputId": "c9b62208-9838-402f-a4bb-66744965103a"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "✅ Clean offer features extracted and merged.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP: Load and prepare transaction data\n",
        "trans = pd.read_parquet(\"/kaggle/input/krish-data/add_trans.parquet\")\n",
        "\n",
        "# Convert 'id2' to string for safe merging\n",
        "trans['id2'] = trans['id2'].astype(str)\n",
        "train['id2'] = train['id2'].astype(str)\n",
        "test['id2'] = test['id2'].astype(str)\n",
        "\n",
        "# Convert relevant columns to correct types\n",
        "trans[\"f367\"] = pd.to_numeric(trans[\"f367\"], errors=\"coerce\")\n",
        "trans[\"f371\"] = pd.to_numeric(trans[\"f371\"], errors=\"coerce\")\n",
        "trans[\"f370\"] = pd.to_datetime(trans[\"f370\"], errors=\"coerce\")\n",
        "\n",
        "# 🛠️ Feature: Time since last transaction\n",
        "latest_trans = trans.groupby(\"id2\")[\"f370\"].max().reset_index()\n",
        "latest_trans[\"days_since_last_transaction\"] = (pd.to_datetime(\"today\") - latest_trans[\"f370\"]).dt.days\n",
        "latest_trans.drop(columns=[\"f370\"], inplace=True)\n",
        "\n",
        "# 🛠️ Feature: Aggregate f367 and f371 (mean, sum, std, max, min)\n",
        "trans_agg = trans.groupby(\"id2\").agg({\n",
        "    \"f367\": ['mean', 'std', 'sum', 'max', 'min'],\n",
        "    \"f371\": ['mean', 'std', 'sum', 'max', 'min']\n",
        "}).reset_index()\n",
        "\n",
        "# Flatten MultiIndex columns\n",
        "trans_agg.columns = ['id2'] + [f\"{col}_{agg}\" for col, agg in trans_agg.columns.tolist()[1:]]\n",
        "\n",
        "# Merge all transaction features\n",
        "trans_features = trans_agg.merge(latest_trans, on=\"id2\", how=\"left\")\n",
        "\n",
        "# Final Merge with train/test\n",
        "train = train.merge(trans_features, on=\"id2\", how=\"left\")\n",
        "test = test.merge(trans_features, on=\"id2\", how=\"left\")\n",
        "\n",
        "print(\"✅ Cleaned and enhanced transaction features merged.\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-03T18:19:46.313619Z",
          "iopub.execute_input": "2025-07-03T18:19:46.313971Z",
          "iopub.status.idle": "2025-07-03T18:20:06.805406Z",
          "shell.execute_reply.started": "2025-07-03T18:19:46.31394Z",
          "shell.execute_reply": "2025-07-03T18:20:06.80456Z"
        },
        "id": "iwqKlULAYJ6o",
        "outputId": "bade3308-6ffe-42b7-b5f4-7069f9e0350b"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "✅ Cleaned and enhanced transaction features merged.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Drop ID columns or unnecessary ones\n",
        "drop_cols = ['id1', 'id3', 'id5', 'y']\n",
        "X = train.drop(columns=[col for col in drop_cols if col in train.columns], errors='ignore')\n",
        "y = train['y']\n",
        "X_test = test[X.columns]  # Align test to train columns\n",
        "\n",
        "# Final shape print\n",
        "print(f\"✅ Final training data shape: {X.shape}\")\n",
        "print(f\"✅ Final test data shape: {X_test.shape}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-03T18:20:06.806396Z",
          "iopub.execute_input": "2025-07-03T18:20:06.806635Z",
          "iopub.status.idle": "2025-07-03T18:20:08.213381Z",
          "shell.execute_reply.started": "2025-07-03T18:20:06.806618Z",
          "shell.execute_reply": "2025-07-03T18:20:08.212317Z"
        },
        "id": "VQ5hTi_RYJ6o",
        "outputId": "10da440b-0be9-47d7-9bcd-b3364566c13a"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "✅ Final training data shape: (770164, 491)\n✅ Final test data shape: (369301, 495)\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "print(\"✅ Train-validation split completed.\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-03T18:20:08.21454Z",
          "iopub.execute_input": "2025-07-03T18:20:08.214907Z",
          "iopub.status.idle": "2025-07-03T18:20:13.138274Z",
          "shell.execute_reply.started": "2025-07-03T18:20:08.214875Z",
          "shell.execute_reply": "2025-07-03T18:20:13.137255Z"
        },
        "id": "2n0j7tU3YJ6p",
        "outputId": "e9758d94-8555-4b52-c1a6-6536d2f513a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "✅ Train-validation split completed.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop identifiers\n",
        "X = train.drop(columns=['y', 'id2', 'id3', 'id4', 'id7'], errors='ignore')\n",
        "y = train['y'].astype(float)\n",
        "\n",
        "# 🚨 Step 1: Remove duplicate columns\n",
        "X = X.loc[:, ~X.columns.duplicated()]\n",
        "\n",
        "# 🚨 Step 2: Sanitize feature names\n",
        "X.columns = (\n",
        "    X.columns\n",
        "    .astype(str)\n",
        "    .str.strip()                     # Remove leading/trailing spaces\n",
        "    .str.replace(r\"[^\\w]+\", \"_\", regex=True)  # Replace special characters with _\n",
        ")\n",
        "\n",
        "# Optional: check if any column is still problematic\n",
        "bad_cols = [col for col in X.columns if any(c in col for c in ['\"', \"'\", '\\\\', '\\n'])]\n",
        "if bad_cols:\n",
        "    print(\"⚠️ Still problematic column names:\", bad_cols)\n",
        "else:\n",
        "    print(\"✅ Column names cleaned.\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-03T18:20:13.139188Z",
          "iopub.execute_input": "2025-07-03T18:20:13.139483Z",
          "iopub.status.idle": "2025-07-03T18:20:15.129615Z",
          "shell.execute_reply.started": "2025-07-03T18:20:13.139461Z",
          "shell.execute_reply": "2025-07-03T18:20:15.128817Z"
        },
        "id": "ZxPFViHQYJ6p",
        "outputId": "1b603108-6083-4061-8982-8c6d3e981287"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "✅ Column names cleaned.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from lightgbm import early_stopping, log_evaluation\n",
        "\n",
        "# -------------------------------\n",
        "# ✅ Drop identifier columns\n",
        "# -------------------------------\n",
        "X = train.drop(columns=['y', 'id2', 'id3', 'id4', 'id7'], errors='ignore')\n",
        "y = train['y'].astype(int)  # Must be 0 or 1 for classification\n",
        "\n",
        "# -------------------------------\n",
        "# ✅ Remove duplicate column names (e.g., engagement_score)\n",
        "# -------------------------------\n",
        "X = X.loc[:, ~X.columns.duplicated()]\n",
        "\n",
        "# -------------------------------\n",
        "# ✅ Sanitize column names (fix special characters for LightGBM)\n",
        "# -------------------------------\n",
        "X.columns = X.columns.str.replace(r\"[^\\w]\", \"_\", regex=True)\n",
        "\n",
        "# -------------------------------\n",
        "# ✅ Split into train/validation\n",
        "# -------------------------------\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# -------------------------------\n",
        "# ✅ Match sanitization in validation set\n",
        "# -------------------------------\n",
        "X_val.columns = X_train.columns  # Ensure same sanitized names\n",
        "\n",
        "# -------------------------------\n",
        "# ✅ Train LightGBM classifier\n",
        "# -------------------------------\n",
        "model = lgb.LGBMClassifier(\n",
        "    objective='binary',\n",
        "    learning_rate=0.05,\n",
        "    num_leaves=128,\n",
        "    max_depth=-1,\n",
        "    feature_fraction=0.8,\n",
        "    bagging_fraction=0.8,\n",
        "    bagging_freq=5,\n",
        "    n_estimators=1000,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "model.fit(\n",
        "    X_train, y_train,\n",
        "    eval_set=[(X_val, y_val)],\n",
        "    callbacks=[\n",
        "        early_stopping(stopping_rounds=50),\n",
        "        log_evaluation(period=100)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# -------------------------------\n",
        "# ✅ Validation Prediction and AUC\n",
        "# -------------------------------\n",
        "val_preds = model.predict_proba(X_val)[:, 1]\n",
        "val_score = roc_auc_score(y_val, val_preds)\n",
        "print(f\"✅ Validation ROC AUC: {val_score:.5f}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-03T18:20:15.130488Z",
          "iopub.execute_input": "2025-07-03T18:20:15.130786Z",
          "iopub.status.idle": "2025-07-03T18:24:59.517371Z",
          "shell.execute_reply.started": "2025-07-03T18:20:15.130765Z",
          "shell.execute_reply": "2025-07-03T18:24:59.516234Z"
        },
        "id": "2a_rV2LRYJ6p",
        "outputId": "9d4f4f5e-b525-4e93-927f-3ecde76452cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n[LightGBM] [Info] Number of positive: 29702, number of negative: 586429\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.882764 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 62453\n[LightGBM] [Info] Number of data points in the train set: 616131, number of used features: 405\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.048207 -> initscore=-2.982837\n[LightGBM] [Info] Start training from score -2.982837\nTraining until validation scores don't improve for 50 rounds\n[100]\tvalid_0's binary_logloss: 0.0775969\n[200]\tvalid_0's binary_logloss: 0.0742437\n[300]\tvalid_0's binary_logloss: 0.0730113\n[400]\tvalid_0's binary_logloss: 0.0723582\n[500]\tvalid_0's binary_logloss: 0.0720636\nEarly stopping, best iteration is:\n[526]\tvalid_0's binary_logloss: 0.0720437\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n✅ Validation ROC AUC: 0.95625\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# ✅ Drop identifier columns from test\n",
        "# -------------------------------\n",
        "X_test = test.drop(columns=['y', 'id2', 'id3', 'id4', 'id7'], errors='ignore')\n",
        "\n",
        "# -------------------------------\n",
        "# ✅ Remove duplicate columns if any\n",
        "# -------------------------------\n",
        "X_test = X_test.loc[:, ~X_test.columns.duplicated()]\n",
        "X_train = X_train.loc[:, ~X_train.columns.duplicated()]\n",
        "\n",
        "# -------------------------------\n",
        "# ✅ Align test columns with train columns\n",
        "# -------------------------------\n",
        "X_test = X_test.copy()\n",
        "missing_cols = [col for col in X_train.columns if col not in X_test.columns]\n",
        "extra_cols = [col for col in X_test.columns if col not in X_train.columns]\n",
        "\n",
        "# Add missing columns as 0\n",
        "for col in missing_cols:\n",
        "    X_test[col] = 0\n",
        "\n",
        "# Drop any extra columns\n",
        "X_test.drop(columns=extra_cols, inplace=True)\n",
        "\n",
        "# Ensure exact same column order\n",
        "X_test = X_test[X_train.columns]\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-03T18:24:59.518646Z",
          "iopub.execute_input": "2025-07-03T18:24:59.519013Z",
          "iopub.status.idle": "2025-07-03T18:25:02.835862Z",
          "shell.execute_reply.started": "2025-07-03T18:24:59.518977Z",
          "shell.execute_reply": "2025-07-03T18:25:02.835111Z"
        },
        "id": "0F2X-evUYJ6q"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# ✅ Evaluate using ROC AUC on validation\n",
        "# -------------------------------\n",
        "val_preds = model.predict_proba(X_val)[:, 1]  # Get probability for class 1\n",
        "val_score = roc_auc_score(y_val, val_preds)\n",
        "print(f\"✅ Validation ROC AUC: {val_score:.5f}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-03T18:25:02.836843Z",
          "iopub.execute_input": "2025-07-03T18:25:02.837125Z",
          "iopub.status.idle": "2025-07-03T18:25:08.670177Z",
          "shell.execute_reply.started": "2025-07-03T18:25:02.837104Z",
          "shell.execute_reply": "2025-07-03T18:25:08.669177Z"
        },
        "id": "m-9IyJeAYJ6q",
        "outputId": "7c39ffb1-102c-4ab7-c1a8-8ad3e3353727"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n✅ Validation ROC AUC: 0.95625\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# ✅ Predict probabilities for test set\n",
        "# -------------------------------\n",
        "test_preds = model.predict_proba(X_test)[:, 1]  # Probability of taking the offer\n",
        "\n",
        "# -------------------------------\n",
        "# ✅ Save to submission file\n",
        "# -------------------------------\n",
        "submission = pd.read_csv('/kaggle/input/krish-data/685404e30cfdb_submission_template.csv')\n",
        "submission['pred'] = test_preds\n",
        "submission.to_csv('/kaggle/working/final_submission.csv', index=False)\n",
        "print(\"✅ Submission saved to /kaggle/working/final_submission.csv\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-03T18:25:08.671361Z",
          "iopub.execute_input": "2025-07-03T18:25:08.671638Z",
          "iopub.status.idle": "2025-07-03T18:25:24.949855Z",
          "shell.execute_reply.started": "2025-07-03T18:25:08.67161Z",
          "shell.execute_reply": "2025-07-03T18:25:24.948852Z"
        },
        "id": "QGjFuUH0YJ6q",
        "outputId": "b6ebe0be-de27-4b89-d0f0-6f253c9d2106"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n✅ Submission saved to /kaggle/working/final_submission.csv\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    }
  ]
}